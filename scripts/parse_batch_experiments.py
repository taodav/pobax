"""
Parse experiment directory from batch_ppo_run.py.
We assume that things are run based off of the runs generated by the files in
scripts/hyperparams.
This file amalgamates all results into a single (huge) tensor, and saves the result
in a file.

After running this, you should run either best_hyperparams.py (for best hyperparams across all envs)
or best_hyperparams_per_env.py (for best hyperparams for each env), to generate
a file that has the best hyperparams, and the best score.
"""
import argparse
from collections import OrderedDict
from copy import deepcopy
import importlib
import os
from pathlib import Path
import pickle
import sys

import jax
import jax.numpy as jnp
import orbax.checkpoint
import numpy as np
from tqdm import tqdm

from pobax.config import PPOHyperparams
from pobax.utils.file_system import get_fn_from_module, get_inner_fn_arguments

from definitions import ROOT_DIR

def combine_seeds_and_envs(x: jnp.ndarray):
    # Here, dim=-1 is the NUM_ENVS parameter. We take the mean over this.
    # dim=-2 is the NUM_STEPS parameter.
    # dim=-3 is the NUM_UPDATES, which is TOTAL_TIMESTEPS // NUM_STEPS // NUM_ENVS.
    # dim=-4 is n_seeds.
    # We take the mean and std_err to the mean over dimensions -1 and -4.
    envs_seeds_swapped = jnp.swapaxes(x, -2, -4).swapaxes(-3, -4)

    # We take the mean over NUM_ENVS dimension.
    mean_over_num_envs = envs_seeds_swapped.mean(axis=-1)
    return mean_over_num_envs


def parse_exp_dir(study_path, study_hparam_path, discounted: bool = False):
    spec = importlib.util.spec_from_file_location('temp', study_hparam_path)
    var_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(var_module)

    all_list_hparams = getattr(var_module, 'hparams')['args']
    assert len(all_list_hparams) == 1
    all_hparams = all_list_hparams[0]
    keys_to_add = [k for k, v in all_hparams.items() if isinstance(v, list)]

    parsed_res = {}

    study_paths = [s for s in study_path.iterdir() if s.is_dir()]

    envs = []
    scores_by_env = {}
    all_swept_hparams_by_env = {}

    for results_path in tqdm(study_paths):
        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()

        restored = orbax_checkpointer.restore(results_path)

        def jnp_to_np(x):
            if isinstance(x, jnp.ndarray):
                return np.array(x)
            return x
        args = jax.tree.map(jnp_to_np, restored['args'])

        def jnp_to_list(x):
            if isinstance(x, jnp.ndarray) or isinstance(x, np.ndarray):
                return x.tolist()
            return x
        swept = jax.tree.map(jnp_to_list, restored['swept_hparams'])
        n_settings = len(list(restored['swept_hparams'].values())[0])

        for key in keys_to_add:
            swept[key] = [args[key]] * n_settings

        if args['env'] not in all_swept_hparams_by_env:
            all_swept_hparams_by_env[args['env']] = {}

        for swept_key, vals in swept.items():
            if swept_key not in all_swept_hparams_by_env[args['env']]:
                all_swept_hparams_by_env[args['env']][swept_key] = []

            all_swept_hparams_by_env[args['env']][swept_key] += vals

        # Get online metrics
        online_eval = restored['out']['metric']
        online_disc_returns = online_eval['returned_episode_returns']
        if discounted:
            online_disc_returns = online_eval['returned_discounted_episode_returns']

        final_eval = restored['out']['final_eval_metric']
        # we take the mean over axis=-2 here, since this dimension might be different
        # for the final eval.
        final_n_episodes = final_eval['returned_episode'].sum(axis=-2, keepdims=True)
        final_disc_returns = final_eval['returned_episode_returns'].sum(axis=-2, keepdims=True)
        if discounted:
            final_disc_returns = final_eval['returned_discounted_episode_returns'].sum(axis=-2, keepdims=True)
        final_disc_returns /= (final_n_episodes + (final_n_episodes == 0).astype(float))  # add the 0 mask to prevent division by 0.

        # we add a num_updates dimension
        final_disc_returns = np.expand_dims(final_disc_returns, -3)

        del restored
        seeds_combined = combine_seeds_and_envs(online_disc_returns)

        final_seeds_combined = combine_seeds_and_envs(final_disc_returns)

        if args['env'] not in scores_by_env:
            scores_by_env[args['env']] = {
                'args': [],
                'fpaths': [],
                'scores': [],
                'final_scores': []
            }
        scores_by_env[args['env']]['args'].append(args)
        scores_by_env[args['env']]['fpaths'].append(results_path)
        scores_by_env[args['env']]['scores'].append(seeds_combined)
        scores_by_env[args['env']]['final_scores'].append(final_seeds_combined)

    dim_ref = ['swept_hparams', 'num_update', 'num_steps', 'seeds']

    for env in scores_by_env.keys():
        scores_by_env[env]['scores'] = jnp.concatenate(scores_by_env[env]['scores'], axis=0)
        scores_by_env[env]['final_scores'] = jnp.concatenate(scores_by_env[env]['final_scores'], axis=0)


    parsed_res = {
        'envs': list(scores_by_env.keys()),
        'swept_hyperparams': all_swept_hparams_by_env,
        'all_hyperparams': all_hparams,
        'dim_ref': dim_ref,
        'scores': scores_by_env
    }
    return parsed_res


def find_file_in_dir(file_name: str, base_dir: Path) -> Path:
    for path in base_dir.rglob('*'):
        if file_name in str(path):
            return path


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('study_path', type=str)
    parser.add_argument('--discounted', action='store_true',
                        help='Do we discount returns?')
    args = parser.parse_args()
    
    study_path = Path(args.study_path).resolve()
    hyperparams_dir = Path(ROOT_DIR, 'scripts', 'hyperparams').resolve()
    study_hparam_filename = study_path.stem + '.py'
    study_hparam_path = find_file_in_dir(study_hparam_filename, hyperparams_dir)

    assert study_hparam_path is not None, f"Could not find {study_hparam_filename} in {hyperparams_dir}"
    print(study_hparam_path)

    if args.discounted:
        parsed_res_file = "parsed_hparam_scores_discounted.pkl"
    else:
        parsed_res_file = "parsed_hparam_scores.pkl"

    parsed_res_path = study_path / parsed_res_file

    parsed_res = parse_exp_dir(study_path, study_hparam_path, discounted=args.discounted)
    parsed_res['discounted'] = args.discounted

    print(f"Saving parsed results to {parsed_res_path}")
    with open(parsed_res_path, 'wb') as f:
        pickle.dump(parsed_res, f, protocol=4)
