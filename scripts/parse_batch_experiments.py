"""
Parse experiment directory from batch_ppo_run.py.
We assume that things are run based off of the runs generated by the files in
scripts/hyperparams.
This file amalgamates all results into a single (huge) tensor, and saves the result
in a file.

After running this, you should run either best_hyperparams.py (for best hyperparams across all envs)
or best_hyperparams_per_env.py (for best hyperparams for each env), to generate
a file that has the best hyperparams, and the best score.
"""
import argparse
import importlib
from pathlib import Path
import pickle

import jax
jax.config.update('jax_platform_name', 'cpu')

import jax.numpy as jnp
import orbax.checkpoint
import numpy as np
from tqdm import tqdm

from pobax.definitions import PROJECT_ROOT_DIR

def mean_over_parallel_envs(x: np.ndarray):
    # Here, dim=-1 is the NUM_ENVS parameter. We take the mean over this.
    # dim=-2 is the NUM_STEPS parameter.
    # dim=-3 is the NUM_UPDATES, which is TOTAL_TIMESTEPS // NUM_STEPS // NUM_ENVS.
    # dim=-4 is n_seeds.
    # we swap dimensions so that n_seeds is -2.
    envs_seeds_swapped = np.swapaxes(x, -2, -4).swapaxes(-3, -4)

    # We take the mean over NUM_ENVS dimension.
    mean_over_num_envs = envs_seeds_swapped.mean(axis=-1)
    return mean_over_num_envs

def find_matching_dict(d, list_d):
    for i, dp in enumerate(list_d):
        if d == dp:
            return i
    return None

def parse_exp_dir(study_path, study_hparam_path, discounted: bool = False):
    spec = importlib.util.spec_from_file_location('temp', study_hparam_path)
    var_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(var_module)

    all_list_hparams = getattr(var_module, 'hparams')['args']
    assert len(all_list_hparams) == 1
    all_hparams = all_list_hparams[0]
    keys_to_add = [k for k, v in all_hparams.items() if isinstance(v, list)]

    parsed_res = {}

    study_paths = [s for s in study_path.iterdir() if s.is_dir()]

    envs = []
    scores_by_env = {}
    all_swept_hparams_by_env = {}
    all_args_no_seeds = []

    # TODO: we need to combine by seeds if seed is a list.
    group_by_seed = isinstance(all_hparams['seed'], list) and len(all_hparams['seed']) > 1

    for results_path in tqdm(study_paths):
        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()

        restored = orbax_checkpointer.restore(results_path)

        def jnp_to_np(x):
            if isinstance(x, jnp.ndarray):
                return np.array(x)
            return x
        args = jax.tree.map(jnp_to_np, restored['args'])

        def jnp_to_list(x):
            if isinstance(x, jnp.ndarray) or isinstance(x, np.ndarray):
                return x.tolist()
            return x
        swept = jax.tree.map(jnp_to_list, restored['swept_hparams'])
        n_settings = len(list(restored['swept_hparams'].values())[0])

        for key in keys_to_add:
            swept[key] = [args[key]] * n_settings

        if args['env'] not in all_swept_hparams_by_env:
            all_swept_hparams_by_env[args['env']] = {}

        args_no_seed = dict(args)
        del args_no_seed['seed']
        matched_i = None
        if group_by_seed:
            matched_i = find_matching_dict(args_no_seed, all_args_no_seeds)

        if matched_i is None:
            if group_by_seed:
                all_args_no_seeds.append(args_no_seed)

            for swept_key, vals in swept.items():
                if swept_key not in all_swept_hparams_by_env[args['env']]:
                    all_swept_hparams_by_env[args['env']][swept_key] = []

                all_swept_hparams_by_env[args['env']][swept_key] += vals
        else:
            # if we find a matching index, we just append the seed
            prev_seed = all_swept_hparams_by_env[args['env']]['seed']
            if not isinstance(prev_seed, list):
                prev_seed = [prev_seed]
            prev_seed.append(args['seed'])
            all_swept_hparams_by_env[args['env']]['seed'] = prev_seed

        # Get online metrics
        online_eval = restored['out']['metric']
        online_disc_returns = online_eval['returned_episode_returns']
        if discounted:
            online_disc_returns = online_eval['returned_discounted_episode_returns']

        final_eval = restored['out']['final_eval_metric']
        # we take the mean over axis=-2 here, since this dimension might be different
        # for the final eval.
        final_n_episodes = final_eval['returned_episode'].sum(axis=-2, keepdims=True)
        final_disc_returns = final_eval['returned_episode_returns'].sum(axis=-2, keepdims=True)
        if discounted:
            final_disc_returns = final_eval['returned_discounted_episode_returns'].sum(axis=-2, keepdims=True)
        final_disc_returns /= (final_n_episodes + (final_n_episodes == 0).astype(float))  # add the 0 mask to prevent division by 0.

        # we add a num_updates dimension
        final_disc_returns = np.expand_dims(final_disc_returns, -3)

        # move n_seeds to last dimension
        del restored
        seeds_combined = mean_over_parallel_envs(online_disc_returns)
        final_seeds_combined = mean_over_parallel_envs(final_disc_returns)

        if args['env'] not in scores_by_env:
            scores_by_env[args['env']] = {
                'args': [],
                'fpaths': [],
                'scores': [],
                'final_scores': []
            }
        # No seed match
        if matched_i is None:
            scores_by_env[args['env']]['args'].append(args)
            scores_by_env[args['env']]['fpaths'].append(results_path)
            scores_by_env[args['env']]['scores'].append(seeds_combined)
            scores_by_env[args['env']]['final_scores'].append(final_seeds_combined)
        else:
            # we found a match, we need to concatenate along -1 for all the scores

            # make a list of things if we need to, then append.
            # for seeds
            seeds = scores_by_env[args['env']]['args'][matched_i]['seed']
            if not isinstance(seeds, list):
                seeds = [seeds]
            seeds.append(args['seed'])
            scores_by_env[args['env']]['args'][matched_i]['seed'] = seeds

            # fpaths
            fpaths = scores_by_env[args['env']]['fpaths'][matched_i]
            if not isinstance(fpaths, list):
                fpaths = [fpaths]
            fpaths.append(results_path)
            scores_by_env[args['env']]['fpaths'][matched_i] = fpaths

            scores = np.concatenate([scores_by_env[args['env']]['scores'][matched_i], seeds_combined], axis=-1)
            scores_by_env[args['env']]['scores'][matched_i] = scores

            final_scores = np.concatenate([scores_by_env[args['env']]['final_scores'][matched_i], final_seeds_combined], axis=-1)
            scores_by_env[args['env']]['final_scores'][matched_i] = final_scores

    dim_ref = ['swept_hparams', 'num_update', 'num_steps', 'seeds']

    for env in scores_by_env.keys():
        scores_by_env[env]['scores'] = np.concatenate(scores_by_env[env]['scores'], axis=0)
        scores_by_env[env]['final_scores'] = np.concatenate(scores_by_env[env]['final_scores'], axis=0)


    parsed_res = {
        'envs': list(scores_by_env.keys()),
        'swept_hyperparams': all_swept_hparams_by_env,
        'all_hyperparams': all_hparams,
        'dim_ref': dim_ref,
        'scores': scores_by_env
    }
    return parsed_res


def find_file_in_dir(file_name: str, base_dir: Path) -> Path:
    for path in base_dir.rglob('*'):
        if file_name in str(path):
            return path


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('study_path', type=str)
    parser.add_argument('--discounted', action='store_true',
                        help='Do we discount returns?')
    args = parser.parse_args()

    study_path = Path(args.study_path).resolve()
    hyperparams_dir = Path(PROJECT_ROOT_DIR, 'scripts', 'hyperparams').resolve()
    study_hparam_filename = study_path.stem + '.py'
    study_hparam_path = find_file_in_dir(study_hparam_filename, hyperparams_dir)

    assert study_hparam_path is not None, f"Could not find {study_hparam_filename} in {hyperparams_dir}"

    if args.discounted:
        parsed_res_file = "parsed_hparam_scores_discounted.pkl"
    else:
        parsed_res_file = "parsed_hparam_scores.pkl"

    parsed_res_path = study_path / parsed_res_file

    parsed_res = parse_exp_dir(study_path, study_hparam_path, discounted=args.discounted)
    parsed_res['discounted'] = args.discounted

    print(f"Saving parsed results to {parsed_res_path}")
    with open(parsed_res_path, 'wb') as f:
        pickle.dump(parsed_res, f, protocol=4)
